{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wKzoYASpy2iI","outputId":"a7e74e5e-6174-4ac0-dfdd-890ee23c229c","executionInfo":{"status":"ok","timestamp":1660737607418,"user_tz":-270,"elapsed":1276648,"user":{"displayName":"Mahmood(Iman) Soltani","userId":"11163060554270252808"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Preparing Data ...\n","Reading data...\n","Reading data Complete!\n","Creating Input Array ...\n","Creating Input Array Complete!\n","Data is Ready ... (sample size:\n","(2164, 700, 4)\n","Epoch 1/25\n","68/68 - 63s - loss: 1.1314 - accuracy: 0.6114 - 63s/epoch - 924ms/step\n","Epoch 2/25\n","68/68 - 58s - loss: 0.4788 - accuracy: 0.8480 - 58s/epoch - 854ms/step\n","Epoch 3/25\n","68/68 - 57s - loss: 0.3045 - accuracy: 0.9067 - 57s/epoch - 843ms/step\n","Epoch 4/25\n","68/68 - 57s - loss: 0.2263 - accuracy: 0.9288 - 57s/epoch - 844ms/step\n","Epoch 5/25\n","68/68 - 58s - loss: 0.2118 - accuracy: 0.9316 - 58s/epoch - 847ms/step\n","Epoch 6/25\n","68/68 - 57s - loss: 0.1956 - accuracy: 0.9358 - 57s/epoch - 837ms/step\n","Epoch 7/25\n","68/68 - 57s - loss: 0.2288 - accuracy: 0.9205 - 57s/epoch - 841ms/step\n","Epoch 8/25\n","68/68 - 56s - loss: 0.2314 - accuracy: 0.9191 - 56s/epoch - 826ms/step\n","Epoch 9/25\n","68/68 - 56s - loss: 0.1720 - accuracy: 0.9413 - 56s/epoch - 831ms/step\n","Epoch 10/25\n","68/68 - 58s - loss: 0.1623 - accuracy: 0.9473 - 58s/epoch - 851ms/step\n","Epoch 11/25\n","68/68 - 57s - loss: 0.1595 - accuracy: 0.9473 - 57s/epoch - 846ms/step\n","Epoch 12/25\n","68/68 - 57s - loss: 0.1537 - accuracy: 0.9519 - 57s/epoch - 838ms/step\n","Epoch 13/25\n","68/68 - 57s - loss: 0.1445 - accuracy: 0.9547 - 57s/epoch - 844ms/step\n","Epoch 14/25\n","68/68 - 56s - loss: 0.1408 - accuracy: 0.9552 - 56s/epoch - 829ms/step\n","Epoch 15/25\n","68/68 - 57s - loss: 0.1484 - accuracy: 0.9469 - 57s/epoch - 840ms/step\n","Epoch 16/25\n","68/68 - 57s - loss: 0.1662 - accuracy: 0.9399 - 57s/epoch - 842ms/step\n","Epoch 17/25\n","68/68 - 58s - loss: 0.1816 - accuracy: 0.9390 - 58s/epoch - 847ms/step\n","Epoch 18/25\n","68/68 - 58s - loss: 0.1742 - accuracy: 0.9385 - 58s/epoch - 854ms/step\n","Epoch 19/25\n","68/68 - 57s - loss: 0.1221 - accuracy: 0.9584 - 57s/epoch - 834ms/step\n","Epoch 20/25\n","68/68 - 57s - loss: 0.1088 - accuracy: 0.9621 - 57s/epoch - 836ms/step\n","Epoch 21/25\n","68/68 - 57s - loss: 0.1111 - accuracy: 0.9603 - 57s/epoch - 835ms/step\n","Epoch 22/25\n","68/68 - 57s - loss: 0.1128 - accuracy: 0.9607 - 57s/epoch - 840ms/step\n","Epoch 23/25\n","68/68 - 57s - loss: 0.1228 - accuracy: 0.9589 - 57s/epoch - 835ms/step\n","Epoch 24/25\n","68/68 - 57s - loss: 0.0997 - accuracy: 0.9640 - 57s/epoch - 833ms/step\n","Epoch 25/25\n","68/68 - 56s - loss: 0.1173 - accuracy: 0.9612 - 56s/epoch - 831ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n","WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd7aff426d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n","WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd7d12f2510> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n","WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd7cf705690> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"]},{"output_type":"stream","name":"stdout","text":["[0.6003047227859497, 0.8041775226593018]\n"]}],"source":["from os import system\n","from sklearn.model_selection import KFold\n","import matplotlib.pyplot as plt\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Dropout\n","from keras.layers import Bidirectional\n","from keras.layers import Attention\n","import numpy as np\n","import sys\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import StandardScaler\n","from pickle import dump\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","np.set_printoptions(threshold=sys.maxsize)\n","# from scipy.signal import argrelextrema\n","# from statsmodels.nonparametric.kernel_regression import KernelReg\n","def find_MaxtimeStep(df):\n","    x = df.loc[df['TREND'].notnull()].index.tolist()\n","    max_distance =0\n","    for i in range (0,len(x)-1):\n","        if x[i+1]-x[i] >max_distance:\n","            max_distance = x[i+1]-x[i]\n","    return max_distance\n","def get_fixedSizeCandle(candles,md):\n","    candles_array = np.array(candles)\n","    Fixed_MaxSizeCandles = np.zeros((md,candles_array.shape[1]),dtype=float)\n","    Fixed_MaxSizeCandles[md-candles_array.shape[0]:,:candles_array.shape[1]] =candles_array\n","    return Fixed_MaxSizeCandles\n","\n","def PreparetestData(path,Max_TimeStep):\n","    print ('Reading data...')\n","    excel_data = pd.read_excel(path)\n","    df = pd.DataFrame(excel_data, columns=['DATE', 'TIME', 'OPEN','HIGH','LOW','CLOSE','TREND'])\n","    print ('Reading data Complete!')\n","    print ('Creating Input Array ...')\n","\n","    x=list()\n","    y=list()\n","    i=0\n","    candles = list()\n","    while i<len(df):\n","        candle =list()\n","        candle = np.zeros(4)\n","        if not(pd.isnull(df.at[i,'TREND'])):\n","            y.append(df.at[i,'TREND'])\n","            if len(candles)>0:\n","                    x.append(get_fixedSizeCandle(candles,Max_TimeStep))\n","            candles = list()\n","        else:\n","          for j in range (0,4):\n","              candle[j] = df.iloc[i,j+2] -df.iloc[i-1,j+2]\n","            #candle[j] = df.iloc[i,j+2]\n","        candles.append(candle)\n","        i = i+1\n","    x.append(get_fixedSizeCandle(candles,Max_TimeStep))\n","    x = np.array(x)\n","    y = np.array(y)\n","    # scaler = StandardScaler()\n","    # for i in range(0,x.shape[0]):\n","    #     x[i]= scaler.fit_transform(x[i])\n","    #x = scaler.fit_transform(x.reshape(-1, x.shape[-1])).reshape(x.shape)\n","    label_encoder = LabelEncoder()\n","    integer_encoded = label_encoder.fit_transform(y)\n","    onehot_encoder = OneHotEncoder(sparse=False)\n","    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n","    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n","    print ('Creating Input Array Complete!')\n","    return np.array(x),np.array(onehot_encoded)\n","\n","def PrepareData(path,allORnum=0):\n","    print ('Reading data...')\n","    excel_data = pd.read_excel(path)\n","    df = pd.DataFrame(excel_data, columns=['DATE', 'TIME', 'OPEN','HIGH','LOW','CLOSE','TREND'])\n","    print ('Reading data Complete!')\n","    print ('Creating Input Array ...')\n","\n","    Max_TimeStep = find_MaxtimeStep(df)\n","    x=list()\n","    y=list()\n","    i=0\n","    candles = list()\n","    while i<len(df):\n","        if len(x)> allORnum and allORnum!=0 :\n","          break\n","        candle =list()\n","        candle = np.zeros(4)\n","        if not(pd.isnull(df.at[i,'TREND'])):\n","            y.append(df.at[i,'TREND'])\n","            if len(candles)>0:\n","                    x.append(get_fixedSizeCandle(candles,Max_TimeStep))\n","            candles = list()\n","        else:\n","          for j in range (0,4):\n","              candle[j] = df.iloc[i,j+2] -df.iloc[i-1,j+2]\n","            #candle[j] = df.iloc[i,j+2]\n","        candles.append(candle)\n","        i = i+1\n","\n","    x.append(get_fixedSizeCandle(candles,Max_TimeStep))\n","    x = np.array(x)\n","    y = np.array(y)\n","    scaler = StandardScaler()\n","    # for i in range(0,x.shape[0]):\n","    #   x[i]= scaler.fit_transform(x[i])\n","    #x = scaler.fit_transform(x.reshape(-1, x.shape[-1])).reshape(x.shape)\n","    #dump(scaler, open('/content/drive/MyDrive/Dataset/std_scaler.bin', 'wb'))\n","    label_encoder = LabelEncoder()\n","    integer_encoded = label_encoder.fit_transform(y)\n","    onehot_encoder = OneHotEncoder(sparse=False)\n","    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n","    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n","    print ('Creating Input Array Complete!')\n","    return np.array(x),np.array(onehot_encoded),Max_TimeStep\n","\n","def Create_Model(num_timesteps,num_features):\n","    model = Sequential()\n","    model.add(LSTM(64, return_sequences=True, input_shape=(num_timesteps, num_features)))\n","    model.add(Bidirectional(LSTM(32)))\n","    model.add(Dense(16))\n","    model.add(Dense(6,activation= 'softmax'))\n","    return model\n","\n","def Show_history_plot(history):\n","    plt.plot(history.history['accuracy'])\n","    plt.plot(history.history['val_accuracy'])\n","    plt.title('model accuracy')\n","    plt.ylabel('accuracy')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'test'], loc='upper left')\n","    plt.show()\n","\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('model loss')\n","    plt.ylabel('loss')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'test'], loc='upper left')\n","    plt.show()\n","def Create_Train_Test(x,y,test_Persent):\n","\n","    X_Train = x[:int((100-test_Persent)*x.shape[0]/100),:,:]\n","    X_Test = x[(X_Train.shape[0]):,:,:]\n","    Y_Train = y[:X_Train.shape[0],:]\n","    Y_Test = y[X_Train.shape[0]:,:]\n","    return X_Train,X_Test,Y_Train,Y_Test\n","\n","#set parameters\n","test_Persent = 15\n","Max_TimeStep=0\n","Max_TimeStep1=0\n","print('Preparing Data ...')\n","x,y,Max_TimeStep = PrepareData('/content/drive/MyDrive/Dataset/Final_DataSet_2400.xlsx')\n","#x1,y1, = PreparetestData('/content/drive/MyDrive/Test_Sample.xlsx',Max_TimeStep)\n","X_Train,X_Test,Y_Train,Y_Test = Create_Train_Test(x,y,test_Persent)\n","# scalers = {}\n","# for i in range(X_Train.shape[2]):\n","#     scalers[i] = StandardScaler()\n","#     X_Train[:, :, i] = scalers[i].fit_transform(X_Train[:, :, i]) \n","\n","# for i in range(X_Test.shape[2]):\n","#     X_Test[:, :, i] = scalers[i].transform(X_Test[:, :,i] )\n","scaler = StandardScaler()\n","for i in range(0,X_Train.shape[0]):\n","   X_Train[i]= scaler.fit_transform(X_Train[i])\n","scaler = StandardScaler()\n","for i in range(0,X_Test.shape[0]):\n","   X_Test[i]= scaler.fit_transform(X_Test[i])\n","\n","print('Data is Ready ... (sample size:')\n","print(X_Train.shape)\n","\n","model = Create_Model(Max_TimeStep,X_Train.shape[2])\n","model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n","history= model.fit(X_Train, Y_Train, epochs=25,  verbose=2)\n","model.save('/content/drive/MyDrive/Dataset/8_17_2022_LastModel')\n","#Show_history_plot(history)\n","#testPredict = model.predict(x1)\n","scores =model.evaluate(X_Test,Y_Test,verbose=0)\n","print(scores)\n","# classes = ['DownTrend','SideTrend','UpTrend']\n","# result=list()\n","# print(x1.shape)\n","# for i in range(0,len(testPredict)):\n","#   result.append(classes[np.argmax(testPredict[i])])\n","#   print(classes[np.argmax(testPredict[i])])\n","# excel_data = pd.read_excel('/content/drive/MyDrive/Test_Sample.xlsx')\n","# df = pd.DataFrame(excel_data, columns=['DATE', 'OPEN','HIGH','LOW','CLOSE','TREND'])\n","# df.rename(columns={'DATE': 'Date', 'OPEN': 'Open', 'HIGH': 'High', 'LOW': 'Low', 'CLOSE': 'Close'},inplace=True)\n","# Trend_Index = df.loc[df['TREND'].notnull()].index.tolist()\n","# for i in range(0,len(Trend_Index)-1):\n","#     Trend_df = pd.DataFrame(df.iloc[Trend_Index[i]:Trend_Index[i+1],:])\n","#     Trend_df.index = pd.DatetimeIndex(Trend_df['Date'])\n","#     fplt.plot(\n","#                 Trend_df,\n","#                 type='candle',\n","#                 title=result[i],\n","#                 ylabel='Price ($)'\n","#             )\n","#     fplt.show()\n","#print(model.evaluate(X_Test,Y_Test,verbose=0))\n","\n","\n","# classes = ['D','S','U']\n","# result = list()\n","# for i in range(0,len(testPredict)):\n","#   result.append([classes[np.argmax(Y_Test[i])],classes[np.argmax(testPredict[i])]])\n","# print(result)\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Train_NEWNormalization.ipynb","provenance":[],"authorship_tag":"ABX9TyO76VZmz7LYQn+V4+d6cxNY"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}