{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"K_Fold_LSTM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOKYpHO+7qxCB8WV1q60/ly"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":12,"metadata":{"id":"Wf4_Yp53V2Yr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656224021554,"user_tz":-270,"elapsed":2098071,"user":{"displayName":"Mahmood(Iman) Soltani","userId":"11163060554270252808"}},"outputId":"6a07c484-596d-4c6b-8bbc-bbbe96edefc5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Score for fold 1: loss of 0.2632380425930023; accuracy of 95.99999785423279%\n","Score for fold 2: loss of 0.11648133397102356; accuracy of 95.99999785423279%\n","Score for fold 3: loss of 0.41434624791145325; accuracy of 92.00000166893005%\n","Score for fold 4: loss of 0.1707179844379425; accuracy of 95.99999785423279%\n","Score for fold 5: loss of 0.14679504930973053; accuracy of 95.99999785423279%\n","Score for fold 6: loss of 0.6622216105461121; accuracy of 83.99999737739563%\n","Score for fold 7: loss of 0.31401997804641724; accuracy of 92.00000166893005%\n","Score for fold 8: loss of 0.14988936483860016; accuracy of 95.99999785423279%\n","Score for fold 9: loss of 0.45227551460266113; accuracy of 80.0000011920929%\n","Score for fold 10: loss of 0.11979996412992477; accuracy of 95.83333134651184%\n","------------------------------------------------------------------------\n","Score per fold\n","------------------------------------------------------------------------\n","> Fold 1 - Loss: 0.2632380425930023 - Accuracy: 95.99999785423279%\n","------------------------------------------------------------------------\n","> Fold 2 - Loss: 0.11648133397102356 - Accuracy: 95.99999785423279%\n","------------------------------------------------------------------------\n","> Fold 3 - Loss: 0.41434624791145325 - Accuracy: 92.00000166893005%\n","------------------------------------------------------------------------\n","> Fold 4 - Loss: 0.1707179844379425 - Accuracy: 95.99999785423279%\n","------------------------------------------------------------------------\n","> Fold 5 - Loss: 0.14679504930973053 - Accuracy: 95.99999785423279%\n","------------------------------------------------------------------------\n","> Fold 6 - Loss: 0.6622216105461121 - Accuracy: 83.99999737739563%\n","------------------------------------------------------------------------\n","> Fold 7 - Loss: 0.31401997804641724 - Accuracy: 92.00000166893005%\n","------------------------------------------------------------------------\n","> Fold 8 - Loss: 0.14988936483860016 - Accuracy: 95.99999785423279%\n","------------------------------------------------------------------------\n","> Fold 9 - Loss: 0.45227551460266113 - Accuracy: 80.0000011920929%\n","------------------------------------------------------------------------\n","> Fold 10 - Loss: 0.11979996412992477 - Accuracy: 95.83333134651184%\n","------------------------------------------------------------------------\n","Average scores for all folds:\n","> Accuracy: 92.38333225250244 (+- 5.488396127967197)\n","> Loss: 0.28097850903868676\n","------------------------------------------------------------------------\n"]}],"source":["from sklearn.model_selection import StratifiedKFold\n","import matplotlib.pyplot as plt\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Dropout\n","from keras.layers import Bidirectional\n","from keras.layers import Attention\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import StandardScaler\n","from google.colab import drive\n","drive.mount('/content/drive')\n","# from scipy.signal import argrelextrema\n","# from statsmodels.nonparametric.kernel_regression import KernelReg\n","def find_MaxtimeStep(df):\n","    x = df.loc[df['TREND'].notnull()].index.tolist()\n","    max_distance =0 \n","    for i in range (0,len(x)-1):\n","        if x[i+1]-x[i] >max_distance:\n","            max_distance = x[i+1]-x[i]\n","    return max_distance\n","def get_fixedSizeCandle(candles,md):\n","    candles_array = np.array(candles)\n","    Fixed_MaxSizeCandles = np.zeros((md,candles_array.shape[1]),dtype=float)\n","    Fixed_MaxSizeCandles[md-candles_array.shape[0]:,:candles_array.shape[1]] =candles_array\n","    return Fixed_MaxSizeCandles\n","def PrepareData(selecteddata):\n","    excel_data = pd.read_excel('/content/drive/MyDrive/Dataset/New_Dataset.xlsx')\n","    df = pd.DataFrame(excel_data, columns=['DATE', 'TIME', 'OPEN','HIGH','LOW','CLOSE','TREND'])\n","    Max_TimeStep = find_MaxtimeStep(df)\n","    x=list()\n","    y=list()\n","    i=0\n","    candles = list()\n","    while i<len(df):\n","        candle =list()\n","        candle = np.zeros(4)#0:Open,1:high,2:low,3:close,4:diff(O-C),5:diff(h-l),6:ma(o),7:ma(h),8:ma(l),9:ma(c)\n","        if not(pd.isnull(df.at[i,'TREND'])):\n","            y.append(df.at[i,'TREND'])            \n","            if len(candles)>0:\n","              # MV= pd.DataFrame(candles).rolling(5, min_periods=1).mean().values              \n","              # for k in range(0,len(candles)):\n","              #   for m in range(0,4):\n","              #     candles[k][m+6] = MV[k,m]\n","              x.append(get_fixedSizeCandle(candles,Max_TimeStep))\n","            candles = list()\n","        else:\n","          for j in range (0,4):\n","            candle[j] = df.iloc[i,j+2] -df.iloc[i-1,j+2]\n","            # candle[4] = candle[0]-candle[3]\n","            # candle[5] = candle[1]-candle[2]\n","            #candle[j] = df.iloc[i,j+2]\n","        candles.append(candle)\n","        i = i+1\n","    x.append(get_fixedSizeCandle(candles,Max_TimeStep))\n","    return np.array(x),np.array(y),Max_TimeStep\n","\n","def Create_Model(num_timesteps,num_features):\n","    model = Sequential()\n","    model.add(LSTM(64, return_sequences=True, input_shape=(num_timesteps, num_features)))\n","    model.add(Bidirectional(LSTM(32)))\n","    model.add(Dense(16))\n","    model.add(Dense(5,activation= 'softmax'))\n","    return model\n","\n","def Show_history_plot(history):\n","    plt.plot(history.history['accuracy'])\n","    plt.plot(history.history['val_accuracy'])\n","    plt.title('model accuracy')\n","    plt.ylabel('accuracy')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'test'], loc='upper left')\n","    plt.show()\n","\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('model loss')\n","    plt.ylabel('loss')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'test'], loc='upper left')\n","    plt.show()\n","def Create_Train_Test(x,y,test_Persent):\n","    scaler = StandardScaler()\n","    for i in range(0,x.shape[0]):\n","        x[i]= scaler.fit_transform(x[i])\n","    #x = scaler.fit_transform(x.reshape(-1, x.shape[-1])).reshape(x.shape)\n","    label_encoder = LabelEncoder()\n","    integer_encoded = label_encoder.fit_transform(y)\n","    onehot_encoder = OneHotEncoder(sparse=False)\n","    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n","    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n","        \n","    X_Train = x[:int((100-test_Persent)*x.shape[0]/100),:,:]\n","    X_Test = x[(X_Train.shape[0]):,:,:]\n","    Y_Train = onehot_encoded[:X_Train.shape[0],:]\n","    Y_Test = onehot_encoded[X_Train.shape[0]:,:]\n","    return X_Train,X_Test,Y_Train,Y_Test\n","\n","#set parameters\n","test_Persent = 15\n","Max_TimeStep=0\n","acc_per_fold = []\n","loss_per_fold = []\n","x,y,Max_TimeStep = PrepareData(4)\n","X_Train,X_Test,Y_Train,Y_Test = Create_Train_Test(x,y,test_Persent)\n","scaler = StandardScaler()\n","for i in range(0,x.shape[0]):\n","    x[i]= scaler.fit_transform(x[i])\n","#x = scaler.fit_transform(x.reshape(-1, x.shape[-1])).reshape(x.shape)\n","\n","fold_no=1\n","kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n","for train, test in kfold.split(x, y):\n","  label_encoder = LabelEncoder()\n","  integer_encoded = label_encoder.fit_transform(y[train])\n","  onehot_encoder = OneHotEncoder(sparse=False)\n","  integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n","  Y_Train = onehot_encoder.fit_transform(integer_encoded)\n","\n","  integer_encoded = label_encoder.fit_transform(y[test])\n","  onehot_encoder = OneHotEncoder(sparse=False)\n","  integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n","  Y_Test = onehot_encoder.fit_transform(integer_encoded)\n","\n","  X_Train, X_Test = x[train], x[test]\n","  #Y_Train, Y_Test = y[train], y[test]\n","\n","  model = Create_Model(Max_TimeStep,X_Train.shape[2])\n","  model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n","  history= model.fit(X_Train, Y_Train, epochs=30,validation_split=0.1,  verbose=0)\n","   #Show_history_plot(history)\n","  #testPredict = model.predict(X_Test)\n","  scores =model.evaluate(X_Test,Y_Test,verbose=0)\n","  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n","  acc_per_fold.append(scores[1] * 100)\n","  loss_per_fold.append(scores[0])\n","  fold_no = fold_no + 1\n","\n","print('------------------------------------------------------------------------')\n","print('Score per fold')\n","for i in range(0, len(acc_per_fold)):\n","  print('------------------------------------------------------------------------')\n","  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n","print('------------------------------------------------------------------------')\n","print('Average scores for all folds:')\n","print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n","print(f'> Loss: {np.mean(loss_per_fold)}')\n","print('------------------------------------------------------------------------')\n","# classes = ['D','S','U']\n","# result = list()\n","# for i in range(0,len(testPredict)):\n","#   result.append([classes[np.argmax(Y_Test[i])],classes[np.argmax(testPredict[i])]])\n","# print(result)\n"]}]}